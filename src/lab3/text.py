def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    if casefold:
        text = text.casefold()
    if yo2e:
        text = text.replace('–Å', '–ï').replace('—ë', '–µ')

    for i in ['\t', '\r', '\n', '\f', '\v']:
        text = text.replace(i, ' ')

    while '  ' in text:
        text = text.replace('  ', ' ')

    return text.strip()

# print(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"))
# print(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞"))
# print(normalize("Hello\r\nWorld"))
# print(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "))


def tokenize(text: str) -> list[str]:
    lst = []
    current_word = []
    i = 0
    while i < len(text):
        c = text[i]
        if c.isalnum() or c == '_':
            current_word.append(c)
        elif c == '-':
            if current_word and i + 1 < len(text) and (text[i + 1].isalnum() or text[i + 1] == '_'):
                current_word.append(c)
            else:
                if current_word:
                    lst.append(''.join(current_word))
                    current_word = []
        else:
            if current_word:
                lst.append(''.join(current_word))
                current_word = []
        i += 1
    if current_word:
        lst.append(''.join(current_word))
    return lst

# print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))
# print(tokenize("hello,world!!!"))
# print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))
# print(tokenize("2025 –≥–æ–¥"))
# print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))

def count_freq(tokens: list[str]) -> dict[str, int]:
    d = {}
    for word in tokens:
        if word not in d:
            d[word] = 1
        else:
            d[word] += 1
    return dict(sorted(d.items()))

# print(count_freq(["a","b","a","c","b","a"]))
# print(count_freq(["bb","aa","bb","aa","cc"]))

def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—ã (—Å–ª–æ–≤–æ, —á–∞—Å—Ç–æ—Ç–∞) –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã
    sorted_counts = sorted(freq.items(), key=lambda item: item[1], reverse=True)
    return sorted_counts[:n]

# print(top_n(count_freq(["a","b","a","c","b","a"]), n=2))
# print(top_n(count_freq(["bb","aa","bb","aa","cc"]), n=2))


# normalize
assert normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t") == "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
assert normalize("—ë–∂–∏–∫, –Å–ª–∫–∞") == "–µ–∂–∏–∫, –µ–ª–∫–∞"

# tokenize
assert tokenize("–ø—Ä–∏–≤–µ—Ç, –º–∏—Ä!") == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]
assert tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ") == ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
assert tokenize("2025 –≥–æ–¥") == ["2025", "–≥–æ–¥"]

# count_freq + top_n
freq = count_freq(["a","b","a","c","b","a"])
assert freq == {"a":3, "b":2, "c":1}
assert top_n(freq, 2) == [("a",3), ("b",2)]

# —Ç–∞–π-–±—Ä–µ–π–∫ –ø–æ —Å–ª–æ–≤—É –ø—Ä–∏ —Ä–∞–≤–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ
freq2 = count_freq(["bb","aa","bb","aa","cc"])
assert top_n(freq2, 2) == [("aa",2), ("bb",2)]